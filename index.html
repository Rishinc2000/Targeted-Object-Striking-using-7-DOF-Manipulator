<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Targeted Object Striking for a 7-DoF Manipulator</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <!-- Header / Hero -->
  <header class="hero">
    <h1>Targeted Object Striking for a 7-DoF Manipulator</h1>
    <p class="subtitle">A Residual Learning Approach</p>
    <p class="authors">
      Priyansh Sinha<sup>1</sup>, Rishin Chakraborty<sup>1</sup>, Samarth Brahmbhatt<sup>2</sup>, Nagamanikandan Govindan<sup>1</sup>
    </p>
    <p class="affiliations">
      <sup>1</sup>International Institute of Information Technology, Hyderabad<br>
      <sup>2</sup>Independent Researcher
    </p>
    <p class="date">03 July 2025</p>
  </header>

  <!-- Abstract -->
  <section id="abstract">
    <h2>Abstract</h2>
    <p>
      We introduce a hybrid framework combining a physics-based inverse dynamics solver with a data-driven residual learning model to perform precision striking manipulation on a 7-DoF robot arm. Our method achieves an average real-world positioning error of 1.68 cm—an 81.6% improvement over traditional system identification techniques—demonstrating robust sim-to-real transfer for high-speed non-prehensile tasks.
    </p>
    <p>
      <a href="Residual_learning_of_striking_manipulation.pdf" target="_blank">Download Full Paper (PDF)</a><br>
      <a href="AIR2025_Poster.pptx" target="_blank">Download Poster (PPTX)</a>
    </p>
  </section>

  <!-- Introduction & Motivation -->
  <section id="introduction">
    <h2>1. Introduction & Motivation</h2>
    <p>
      While robotic grasping is ubiquitous, non-prehensile actions such as striking expand the reachable workspace by leveraging high-velocity impacts. Applications include rapid sorting in logistics, workspace clearing in manufacturing, and service robotics tasks. The core challenge lies in accurately modeling impact dynamics under real-world uncertainties.</n>
  </section>

  <!-- Methodology -->
  <section id="methodology">
    <h2>2. Methodology</h2>
    <h3>2.1 Inverse Dynamics Solver</h3>
    <p>
      We use MuJoCo and NLopt to compute ideal striking velocity <em>v<sub>sim</sub></em> and angle <em>θ<sub>sim</sub></em> for a given initial and target position.</p>
    <h3>2.2 Residual Learning</h3>
    <p>
      A neural network (MLP) is trained on 500 hardware strikes to predict residuals Δv and Δθ that correct for unmodeled sim-to-real discrepancies, using inputs: path lengths (P<sub>x</sub>, P<sub>y</sub>), v<sub>sim</sub>, θ<sub>sim</sub>.</p>
    <h3>2.3 Hardware Execution</h3>
    <p>
      Final commands: v<sub>real</sub> = v<sub>sim</sub> + Δv, θ<sub>real</sub> = θ<sub>sim</sub> + Δθ, executed on a uFactory xArm 7-DOF manipulator with a custom low-level controller.</p>
  </section>

  <!-- Results & Findings -->
  <section id="results">
    <h2>3. Results & Key Findings</h2>
    <ul>
      <li><strong>Accuracy Improvement:</strong> Residual learning reduces mean error from 9.12 cm to 1.68 cm (81.6% reduction).</li>
      <li><strong>Generalization:</strong> On a second object (cube), error reduced from 10.02 cm to 4.36 cm.</li>
    </ul>
    <figure>
      <img src="assets/accuracy_plot.png" alt="Error comparison plot">
      <figcaption>Error ellipses comparing system identification vs. residual learning.</figcaption>
    </figure>
  </section>

  <!-- Conclusion & Future Work -->
  <section id="conclusion">
    <h2>4. Conclusion & Future Work</h2>
    <p>
      We present a robust sim-to-real pipeline that leverages residual learning to compensate for unmodeled dynamics in striking manipulation tasks. Future work includes dynamic adaptation for arbitrary object shapes and on-the-fly residual tuning without manual data collection.</p>
  </section>

  <!-- Media Demo -->
  <section id="demo">
    <h2>5. Demo Video</h2>
    <video controls width="640">
      <source src="demo_video.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <p>Add AI voice-over narration via Clipchamp or Murf.ai for automated, high-quality audio explanation.</p>
  </section>

  <!-- Contact -->
  <footer>
    <h2>Contact</h2>
    <p>
      <strong>Priyansh Sinha</strong>: priyansh.sinha@research.iiit.ac.in<br>
      <strong>Rishin Chakraborty</strong>: rishin.chakraborty@research.iiit.ac.in<br>
      <strong>Samarth Brahmbhatt</strong>: samarth.robo@gmail.com<br>
      <strong>Nagamanikandan Govindan</strong>: nagamanikandan.g@iiit.ac.in
    </p>
  </footer>
</body>
</html>
